import torch
from transformers import T5ForConditionalGeneration, T5Tokenizer
model_name = 'cointegrated/rut5-base-multitask'
cache_dir = './my_cache_dir'  # Указываем путь для кэша

tokenizer = T5Tokenizer.from_pretrained(model_name, cache_dir=cache_dir)
model = T5ForConditionalGeneration.from_pretrained(model_name, cache_dir=cache_dir)

def generate(text, **kwargs):
    inputs = tokenizer(text, return_tensors='pt')
    with torch.no_grad():
        hypotheses = model.generate(**inputs, num_beams=5, **kwargs)
    return tokenizer.decode(hypotheses[0], skip_special_tokens=True)
